so let me jot down differences and similarities between my and  hello interview  design of job scheduler 

1.  Both have schedule and schedule job history but naming is different
2. hello interview have partition on hour , rather than job_id in my design , so we have to query only one or two partitions to find job that , great optimisaton I would say  , we can do the same in mysql by creating hourly partitions with 2- shards intact , with hourly partitions inside shard to optimise the reads , and job distribution on shards will be base on user_id 
3.  so we used two crons one to schedule and one to execute but in hello interview basically there is only cron which executes and that execute cron basically schedules next run as well when a job completes hence no need of cron 1 , another great optimistion
4.  so for status it is different so it has assumed that status query will be done by a user rather than specific job , hence it has used GSI on status to query status of jobs created by  user in certain time range , now lets say if we have to adhere to this requirement in mysql what can we do , we can have index on scheduled_at but it would range to all partition , so our query will be able to select correct shard based on user_id as our DB is sharded by that and then it will range over partition in the shard which should be okay as it is the same shard 
5. So now other challenge was to ensure running job within 2 sec , now in my design I have reduced cron frequency , but the issue is just fetching the data and hten pushing it to queue and then executor processing , and also I am fetching jobs whcih are already behind current time , so what hello interview has done it has follow similar architecutre but  taken jobs which are to be shceudled in next 5 mins rather than last 5 seconds (in my design) , this ensures future job are pushed to queueu , now it could be possible that a job that has to be schedueld immediately might get missed up lets say our cron picked up jobs but a new job came to be scheduled under 5 mins then such jobs would be directly sent to queue , but the issue is these jobs would be behind in the queue which mean it may get delayed , but what we can do here is so we use special queue such as AMAZON SQS with deliver delay , so this delay is automatically handled by SQS , we can also use redis sorted set that u suggested but in that comes with challendge of durability , replication other things but that is also a good soln
6. now for atleast one execution there could be possibility that worker itself goes down for that in my design we have kafka queues commit , there wont be any commit , so job will be re-consumed , but in case of SQS we have visibility timeout to handle also this is job itslef while processing we can have delayed retried backed by SQS


Key point is 
SQS and partitioning